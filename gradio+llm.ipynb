{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define your custom package installation path in Google Drive\n",
        "#    You can change 'my_colab_packages' to any folder name you prefer.\n",
        "custom_package_path = '/content/drive/MyDrive/my_colab_packages'\n",
        "\n",
        "# Define a path for storing large language models persistently\n",
        "model_storage_path = '/content/drive/MyDrive/colab_models'\n",
        "\n",
        "# 3. Add the custom package path to Python's system path\n",
        "if custom_package_path not in sys.path:\n",
        "    sys.path.insert(0, custom_package_path)\n",
        "\n",
        "print(f\"Custom package path added to sys.path: {custom_package_path}\")\n",
        "print(f\"Current sys.path: {sys.path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7m0XyQDJiQqW",
        "outputId": "4d45e520-606d-4ddf-da92-838e0c124b24"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Custom package path added to sys.path: /content/drive/MyDrive/my_colab_packages\n",
            "Current sys.path: ['/content/drive/MyDrive/my_colab_packages', '/content', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.12/dist-packages/IPython/extensions', '/root/.ipython']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import pipeline\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline"
      ],
      "metadata": {
        "id": "gZENPrf63QuW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the HuggingFace pipeline using the (downloaded or loaded) model and tokenizer\n",
        "model_id = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
        "local_model_dir = os.path.join(model_storage_path, model_id.split('/')[-1])\n",
        "tokenizer = AutoTokenizer.from_pretrained(local_model_dir)\n",
        "model = AutoModelForCausalLM.from_pretrained(local_model_dir)\n",
        "print(\"Loading complete.\")\n",
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    temperature=0.1, # Reduced for more direct answers\n",
        "    max_new_tokens=30 # Reduced to encourage shorter, exact answers\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "model = ChatHuggingFace(llm=llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAT3F3Ee3daJ",
        "outputId": "f378837c-458e-4b66-dbbc-f151c868759e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "Wt0mWDVVhzrd",
        "outputId": "eb2ff614-7251-4fb1-c1ff-34172654276f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://93b48470fe81bc3fc8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://93b48470fe81bc3fc8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "\n",
        "# Function to generate a response from the model\n",
        "def generate_response(prompt_txt):\n",
        "    raw_response = model.invoke(prompt_txt)\n",
        "    # Extract only the assistant's response and clean up tags\n",
        "    response_content = raw_response.content\n",
        "    if '<|assistant|>' in response_content:\n",
        "        # Split by assistant tag and take the part after it\n",
        "        generated_text = response_content.split('<|assistant|>')[-1].strip()\n",
        "    else:\n",
        "        generated_text = response_content.strip()\n",
        "\n",
        "    # Remove any trailing '</s>' or other potential artifacts\n",
        "    if '</s>' in generated_text:\n",
        "        generated_text = generated_text.split('</s>')[0].strip()\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Create Gradio interface\n",
        "chat_application = gr.Interface(\n",
        "    fn=generate_response,\n",
        "    allow_flagging=\"never\",\n",
        "    inputs=gr.Textbox(label=\"Input\", lines=2, placeholder=\"Type your question here...\"),\n",
        "    outputs=gr.Textbox(label=\"Output\"),\n",
        "    title=\"LLM.ai Chatbot\",\n",
        "    description=\"Ask any question and the chatbot will try to answer.\"\n",
        ")\n",
        "\n",
        "# Launch the app\n",
        "chat_application.launch(server_name=\"127.0.0.1\", server_port= 7868)"
      ]
    }
  ]
}